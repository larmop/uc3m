---
title: "Case Study: ARIMA models "
author: "Lara Monteserín Placer"
date: 'UC3M, 2023/24'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)

```

# HMW 1

### Lara Monteserín Placer

## 1. Introduction

On any given day, websites attract a diverse audience, with numerous visitors engaging with content. Comprehending the factors that influence total visits is essential. In this study, we aim to predict the total number of visits to a specific website by looking for patterns in the data based on the time period over which they have been recorded.

Some of the advantages of predicting future visits are:

-   **Web traffic influences everything:** Visits serve as a fundamental metric, helping to shape the website's content strategy.

-   **Resource allocation for growth:** Allows website administrators to allocate resources efficiently, ensuring optimal performance.

-   **Strategic planning in the digital realm:** By anticipating future visits, website managers can refine user experience, content development and targeted advertising strategies to align with anticipated traffic trends.

![](foto.jpeg){width="565"}

In this case study, we will use a data set with 2167 instances of website visits in cinemas from 2014 to 2020. This file contains 5 years of daily time series data for various traffic measures on a statistical forecasting website.

We took the dataset from Kaggle: <https://www.kaggle.com/datasets/bobnau/daily-website-visitors/>

### 1.1 The dataset: Description of features

The dataset daily-website-visitors.csv includes the following 8 variables:

1.  **Row**: A unique identifier for each row in the dataset.
2.  **Day**: The day of the week represented in text form (e.g., Sunday, Monday).
3.  **Day.Of.Week**: The day of the week represented in numeric form (1-7).
4.  **Date**: The date in mm/dd/yyyy format.
5.  **Page.Loads**: The daily count of pages loaded on the website.
6.  **Unique.Visits**: The daily count of visitors whose IP addresses haven't been associated with any page hits for over 6 hours, indicating unique visits.
7.  **First.Time.Visits**: The count of unique visitors who do not have a cookie identifying them as previous customers, representing the number of first-time visitors.
8.  **Returning.Visits**: The count of unique visitors, excluding first-time visitors, indicating the number of visitors returning to the website.

### 1.2 The goal

Make two different predictions, both based on the column with spatio-temporal information (*Date*), looking for possible patterns in the data.

We will predict the Unique.Visits feature, which represents the unique visits to a web page, and on the other hand the Returning.Visits feature, which represents the visits of those users who have already visited the site.

This way, we will try to forecast how both sequences of observations will continue into the future.

```{r}
library(fpp3) #attach several packages as tidyverse, lubridate, etc. 
library(tidyquant)


setwd("C:/Users/laram/Desktop/Todo/UC3M/Third Bimester/Forecasting/Assignment1")
website <- read.csv("daily-website-visitors.csv")

# Display column names in the dataset
colnames(website)

#dim(website)
#str(website)
#summary(website)
```

## 2. Data Preparation/Pre-processing

### 2.1 Selecting relevant columns

As this is a time-series analysis, we only need the two targets we are going to predict and the column with the time values. For that reason, we will remove all columns except of "Unique.Visits", "Returning.Visits" and "Date".

```{r}
#Delete those columns
cols_to_remove <- c("Day", "Day.Of.Week", "Page.Loads", "First.Time.Visits", "Row")

website <- website[, !(names(website) %in% cols_to_remove)]
```

### 2.2 Data cleaning (NAs and normalization)

We want to ensure that we are working with a reliable and accurate dataset. Until now, we have 2167 instances.

As it has been shown below, there are not any null values in the dataset.

We also normalize the names of the columns so that they easier to work with.

When removing duplicates, we find out that there are no duplicates in the dataset.

```{r}
# Counting NAs and empty values in each column
na_counts <- colSums(is.na(website) | website == "")
na_counts <- na_counts[order(-na_counts)]
#print(na_counts)

#Data types
glimpse(website)

#Data transformation to ensure that there are not any leading and trailing whitespaces from all character columns in the dataframe
library(stringr)
website <- website %>% mutate_all(str_trim)

#Standardizing and cleaning column names. Useful for ensuring that column names are consistent and easy to work with.
library(janitor)
website <- website%>% clean_names() 

#Creates a logical vector duplicate_rows indicating whether each row in the data frame is a duplicate of a previous row
duplicate_rows <- website %>% duplicated.data.frame()

#Retains only the unique rows
library(dplyr)
website <- website %>% distinct(date, .keep_all = TRUE)

#Number of rows after removing duplicates
n_rows <- nrow(website)
print(n_rows)
```

### 2.3 Data cleaning (looking for outliers)

```{r}
# Convert 'unique_visits' and 'returning_visits' to numeric after removing commas
website$unique_visits <- as.numeric(gsub(",", "", website$unique_visits))
website$returning_visits <- as.numeric(gsub(",", "", website$returning_visits))

# Calculate Z-scores for 'unique_values' and 'returning_values'
z_scores_unique <- scale(website$unique_visits)
z_scores_returning <- scale(website$returning_visits)

# Set a threshold to consider a value as an outlier
threshold <- 2

# Identify outliers in 'unique_values'
outliers_unique <- which(abs(z_scores_unique) > threshold)

# Identify outliers in 'returning_values'
outliers_returning <- which(abs(z_scores_returning) > threshold)

# Show the indices of outliers
cat("Outliers in unique_values:", outliers_unique, "\n")
cat("Outliers in returning_values:", outliers_returning, "\n")
```

As we have identified some outliers, we are going to see the distribution of data, in case that it help us to make some transformations that can reduce their impact.

```{r}
hist(website$unique_visits)
hist(website$returning_visits)
```

Both distributions are centered, so no transformations are needed.

Anyway we have also created the following boxplots to see the range of values of both targets:

```{r}
boxplot(website$unique_visits, outline = FALSE, main = "Unique visits Boxplot")

boxplot(website$returning_visits, outline = FALSE, main = "Returning visits Boxplot")

```

And also two scatter plots highlighting the outliers in red

```{r}
library(ggplot2)

ggplot(website, aes(x = date, y = unique_visits)) +
  geom_point(aes(color = ifelse(unique_visits %in% outliers_unique, "Outlier", "Not Outlier"))) +
  scale_color_manual(values = c("Outlier" = "red", "Not Outlier" = "black"), guide = "none") +
  labs(title = "Scatter Plot with Outliers Highlighted", x = "Date", y = "Unique Values") +
  theme_minimal()
```

```{r}
ggplot(website, aes(x = date, y = returning_visits)) +
  geom_point(aes(color = ifelse(returning_visits %in% outliers_returning, "Outlier", "Not Outlier"))) +
  scale_color_manual(values = c("Outlier" = "red", "Not Outlier" = "black"), guide = "none") +
  labs(title = "Scatter Plot with Outliers Highlighted", x = "Date", y = "Returning Values") +
  theme_minimal()
```

In the context of website visits, occasional spikes or unusual patterns might be expected. For example, outliers could be due to special events, marketing campaigns, or other factors that lead to a sudden increase in traffic. Now that we know where they are, we will have this into account when visualizing data, in order to find patterns.

## 3. Data visualization (trends and seasonality).

In this section we will look for time-series patterns: trends, cycles, seasonality and volatility. Start with trends, then seasons, see if there are cycles and they say if the noise is high o r not

After adjusting for seasonality, we can identify better the trends or cycles

Seasonal and cyclic patterns are not the same:

\- seasonal pattern constant length, cyclic pattern variable length

\- average length of cycle longer than length of seasonal pattern

\- magnitude of cycle more variable than magnitude of seasonal pattern

\- the timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data

Trends: long-term increase or decrease. Count the peaks. A peak every 4 quarters. People have more beers in summer than in winter. To predict well, considering the temperature is important (as we see that there is a correlation). The seasonality is a year and the period is 4.

Cycles: rises and falls that are not of a fixed period (a pattern that cannot be predicted). Data is quarterly, so every 3 months there is a quarter.

Seasonality: deterministic pattern (fixed and known length) from a season (every monday, for example \--\> a pattern that can be predicted)

Volatility: size of noise

5)  Will we forecast well or not?

The variables have complex seasonality that is keyed to the day of the week and to the academic calendar. The patterns you you see here are similar in principle to what you would see in other daily data with day-of-week and time-of-year effects. Some good exercises are to develop a 1-day-ahead forecasting model, a 7-day ahead forecasting model, and an entire-next-week forecasting model (i.e., next 7 days) for unique visitors.

```{r}
library(tidyquant)

# Convert 'date' column to Date format
website$date <- as.Date(website$date, format = "%m/%d/%Y")

# Graficar usando ggplot2 y tidyquant
plot_website <- website %>%
  ggplot(aes(x = date, y = unique_visits)) +
  geom_line(col = "darkblue", size = 1) +
  labs(title = "Daily Unique Visits",
       x = "Date", y = "Unique Visits") +
  theme_tq()

# Mostrar el gráfico
print(plot_website)

# Graficar usando ggplot2 y tidyquant
plot_website2 <- website %>%
  ggplot(aes(x = date, y = returning_visits)) +
  geom_line(col = "darkblue", size = 1) +
  labs(title = "Daily Returning Visits",
       x = "Date", y = "Returning Visits") +
  theme_tq()

# Mostrar el gráfico
print(plot_website2)
```

```{r}
website$lag_unique_visits <- lag(website$unique_visits, 250)

# Plotting
plot_website_lagged <- website %>%
  ggplot(aes(x = date, y = lag_unique_visits)) +
  geom_line(col = "darkblue", size = 1) +
  labs(title = "Lagged Unique Visits",
       x = "Date", y = "Lagged Unique Visits") +
  theme_tq()

# Show the plot
print(plot_website_lagged)
```

```{r}
# Assuming 'website' is your dataset
website %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = unique_visits), color = "blue") +
  geom_line(aes(y = returning_visits), color = "red") +
  labs(title = "Time Series Plot for Targets",
       x = "Time", y = "Target Values") +
  theme_minimal()

```

```{r}
# Assuming 'website' is your dataset
website %>%
  ggplot(aes(x = unique_visits, y = returning_visits)) +
  geom_point() +
  labs(title = "Scatter Plot for Targets",
       x = "Unique Values", y = "Returning Values") +
  theme_minimal()

```

Graph the data and try to answer:

\- Are there consistent patterns?

\- Is there a significant trend?

\- Is seasonality important?

\- Is there evidence of the presence of cycles?

\- Are there significant changes in volatility?

\- Are there any outliers in the data?

\- How strong are the relationships among the variables?

First insights:

\- Frequency (hourly, daily, monthly, etc.)

\- Forecasting horizon (one-day ahead, 7-days ahead, etc.)

\- Features (from the own target, and other variables)

\- Performance measures (mse, mape, based on costs, etc.)

\- Prediction intervals

\- Models (traditional, ML, etc.)

```{r}
library(tsibble) # Para autoplot

# Asumiendo que quieres trabajar con unique_visits
website <- website |> mutate(unique_visits = as.numeric(gsub(",", "", unique_visits))) # Convertir a numérico
website_ts <- website |> as_tsibble(index = date) # Convertir a tsibble

website_ts |> autoplot(unique_visits)
```

```{r}
# Asumiendo que quieres trabajar con returning_visits
website <- website |> mutate(returning_visits = as.numeric(gsub(",", "", returning_visits))) # Convertir a numérico
website_ts <- website |> as_tsibble(index = date) # Convertir a tsibble

website_ts |> autoplot(returning_visits)

```

Para seasonalities:

```{r}
library(feasts)

#Convertir a tsibble
website_ts <- website %>% as_tsibble(index = date)

# Para visualizar estacionalidad en 'unique_visits'
website_ts %>% gg_season(unique_visits, labels = "right") + labs(title = "Estacionalidad de Visitas Únicas")

# O para 'returning_visits'
website_ts %>% gg_season(returning_visits, labels = "right") + labs(title = "Estacionalidad de Visitas Recurrentes")
```

Multiple seasonal periods:

Day of the week:

```{r}
website_ts |> gg_season(unique_visits, period = "week")
website_ts |> gg_season(returning_visits, period = "week")

```

Academic calendar:

```{r}
website_ts |> gg_season(unique_visits, period = "year")
website_ts |> gg_season(returning_visits, period = "year")
```

Seasonal Subseries Plot:

```{r}
website_ts |> gg_subseries(returning_visits)
```

### Autocorrelations and lags

#### ACF

Lags: aquí 7 lags son 1 semana

For lag 4 (4 periods), correlation is very clear. It makes sense as 4 periods correspond to a year (and the seasonality was year).

Here lag 8 correlation is very high because of lag's 4 correlation, but sometimes lag 8's correlation could be independent of lag's 4 correlation.

The autocorrelation function is used to compute all these correlations. Used to create models.

```{r}
website_ts |> gg_lag(unique_visits, geom = "point")
website_ts |> gg_lag(returning_visits, geom = "point")

```

If there is seasonality, the ACF at the seasonal lag (e.g., 12 for monthly data) will be large and positive.

Autocorrelation:

```{r}
website_ts |> ACF(unique_visits) |> autoplot()
website_ts |> ACF(returning_visits) |> autoplot()

```

When data have a trend, the autocorrelations for small lags tend to be large and positive

\- When data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e., at multiples of the seasonal frequency)

\- When data are trended and seasonal, you see a combination of these effects

```{r}
website_ts |> ACF(unique_visits - lag(unique_visits), lag_max = 48) |> autoplot()

website_ts |> ACF(returning_visits - lag(returning_visits), lag_max = 48) |> autoplot()

```

#### PACF

PACF measures the linear relationship between the correlations of the residuals, hence removes the dependence of lags on other lags.

```{r}
website_ts |> PACF(unique_visits, lag_max = 48) |> autoplot()
website_ts |> PACF(returning_visits, lag_max = 48) |> autoplot()

```

```{r}
website_ts <- website |> mutate(return = difference(unique_visits)/lag(unique_visits))

```

#### Time-Series decomposition

In order to get the trend, we can compute the average (apply filtering) using loesg from ggplot. The function is only based in the past data.

STL(Employed \~ trend(6)) \--\> it is 6 by default. It represents the number of periods that R is using to compute the average/smooth the function. The biggest, the smoother.

```{r}
website_ts <- website %>% as_tsibble(index = date)

dcmp <- website_ts |>
  model(stl = STL(unique_visits))
components(dcmp) |> autoplot()
```

```{r}
website_model <- website_ts %>%
  model(stl = STL(unique_visits ~ season(window = "periodic")))
components(website_model) |> autoplot()
```

```{r}
website_ts |>
  autoplot(unique_visits, color = "gray") +
  autolayer(components(dcmp), trend, color = "#D55E00") +
  labs(y = "Visits to website", title = "Total visits")
```
