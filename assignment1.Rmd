---
title: "Case Study: Naive Bayes application "
author: "Lara Monteserín Placer"
date: 'UC3M, 2023/24'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)

```

# Assignment 1

### Lara Monteserín Placer

Every day, millions of users express their thoughts and emotions through tweets, creating a vast pool of valuable data. Analyzing the sentiment behind these tweets provides insights into public opinions, attitudes, and feelings.

In this study, our goal is to harness the power of sentiment analysis by building a classifier to predict whether a tweet carries a positive, negative, or neutral sentiment based on the words and expressions used by the users. This will be done through a brief naive Bayes analysis.

Some of the advantages of being able to classify tweets depending on its sentiment are:

1.  **Public Opinion Monitoring:** It offers real-time insights into public opinion on various topics, products, or events.

2.  **Brand Perception:** If performed around a brand on social media, it aids businesses in reinforcing loyalty with positive mentions and managing negative sentiments for effective reputation management.

3.  **Customer Feedback Analysis:** It provides companies with valuable insights into customer experiences, facilitating data-driven improvements in products and services.

4.  **Market Trends and Predictions:** It helps to identify emerging trends and anticipate shifts in public sentiment, empowering in making informed decisions.

5.  **Public Response to Events:** Crucial for public relations and crisis management, it offers a quick and efficient way to gauge how people react to specific events or announcements on social media.

![](foto.avif)

In this case study, we will use a dataset with 162961 instances of tweets published in India during the year 2021.

## 1. Introduction to the data

We took the dataset from Kaggle: <https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset>

The dataset Twitter_Data.csv includes the following 2 variables:

1.  **clean_text**: The complete text from the tweet. The type of the data is character.

2.  **category**: A unique code assigned to each text, indicating the sentiment code. The type of data is integer. There are 3 types of sentiments to classify the tweets in:

    -   Positive: 1

    -   Neutral: 0

    -   Negative: -1

```{r}
#To not show warnings
options(warn = -1)

#Clear the system.
rm(list=ls())

library(tidyverse)
library(MASS)
library(e1071)
library(caret)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)

setwd("C:/Users/laram/Desktop/Todo/UC3M/Third Bimester/Bayesian Learning/Assignment 1")

twitter <- read.csv("Twitter_Data.csv")

# Display column names in the dataset
colnames(twitter)

dim(twitter)
str(twitter)
summary(twitter)
```

## 2. Data cleaning analysis

We prepare the dataset for statistical analysis. We want to ensure that we are working with a reliable and accurate dataset.

Until now, we have 162980 instances.

The 2 columns have null values, 7 in category and 4 in clean_text. As this value is very low compared to the size of the dataset, we will just remove those instances, so we keep 162973 instances.

When first removing duplicates, we find out that there are just 2 duplicated values. Also, if we count the unique values in clean_text, there are only 162969, so these are the values that we keep to perform the statistical analysis.

After that, we turn the tweets into a corpus in order to work with them. From here, we will clean the corpus: put everything in lower case. remove any numbers and punctuation, take out uninformative stop words and remove excess white space.

Finally, we extract the indices (pos_indices, neu_indices and neg_indices) that point to the positions of rows where the category is considered positive, neutral or negative, respectively.

```{r}
# Counting NAs and empty values in each column
na_counts <- colSums(is.na(twitter) | twitter == "")
na_counts <- na_counts[order(-na_counts)]
print(na_counts)

# Removing instances with null values
twitter <- twitter[complete.cases(twitter), ]

#Current number of rows
n_rows <- nrow(twitter)
print(n_rows)

#Creates a logical vector duplicate_rows indicating whether each row in the data frame is a duplicate of a previous row
duplicate_rows <- twitter %>% duplicated.data.frame()

#Retains only the unique rows
library(dplyr)
twitter <- distinct(twitter)

#Number of rows after removing duplicates
n_rows2 <- nrow(twitter)
print(n_rows2)

# Duplicates based on column "clean_text"
twitter <- twitter %>% distinct(clean_text, .keep_all = TRUE)

n_rows3 <- nrow(twitter)
print(n_rows3)

# Turn the messages into a corpus.
corpus <- Corpus(VectorSource(twitter$clean_text))
inspect(corpus[1:5])

## CLEANING

# Put everything in lower case.
clean_corpus <- tm_map(corpus, tolower)
inspect(clean_corpus[1:5])

# Remove any numbers.
clean_corpus <- tm_map(clean_corpus, removeNumbers)
inspect(clean_corpus[1:5])

# Remove punctuation.
clean_corpus <- tm_map(clean_corpus, removePunctuation)
inspect(clean_corpus[1:5])

# Take out uninformative stop words.
stopwords("en")[1:10]
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords("en"))
inspect(clean_corpus[1:5])

# Remove excess white space.
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
inspect(clean_corpus[1:5])

# Check which tweets are positive, which neutral and which are negative.

pos_indices <- which(twitter$category == 1)
pos_indices[1:3]

neu_indices <- which(twitter$category == 0)
neu_indices[1:3]

neg_indices <- which(twitter$category == -1)
neg_indices[1:3]
```

## 3. Separate the data into a training set and a test set

It is always a good idea to separate from the beginning the training set (what the tool is going to see) from the testing set (used only to validate predictions).

```{r}
# split between training and testing sets
spl = createDataPartition(twitter$category, p = 0.8, list = FALSE)

twTrain = twitter[spl,]
twTest = twitter[-spl,]

corpus_train <- clean_corpus[spl]
corpus_test <- clean_corpus[-spl]

#str(twTrain)
#summary(twTrain)


```

## 4. Use word clouds

We plot the three clouds corresponding to the 3 types of sentiment that can be extracted from the tweets, in order to look at the different categories and explain any obvious differences between them.

As we are working with a huge number of instances, we need to establish a big number for the minimum frequency of the words to appear in the word cloud. This is the reason why we have chosen 1500 words as the minimum in all texts together.

For all indices, "Modi" is the most repeated word by far, that is the surname of the Prime Minister of India.

For the positive indices, some words stand out apart from "Modi", just like: "India", "best", "win", "love", "please" "power" or "thanks", "good", "great", "happy".

For the neutral indices, the most repeated words are "congress", "bjp" (that is a political party in India), "vote", "minister", "nation" or "country", that are at the same time common with the positive and the negative indices in some cases.

For the negative indices, the words that stand out the most are "poor", "hate", "people", "fake", "money" or "time".

```{r}
par(mar=c(2,2,2,2))  # Set smaller margins
wordcloud(clean_corpus[pos_indices], min.freq=1500, scale=c(3,.5))
wordcloud(clean_corpus[neu_indices], min.freq=1500, scale=c(3,.5))
wordcloud(clean_corpus[neg_indices], min.freq=1500, scale=c(3,.5))
```

## \* Something to point out

The dataset we have chosen is that big that the function convert_count_tw cannot be applied to the train and the test sets because there is a problem:

*Error: cannot allocate vector of size X Gb*

As this assignment is not about applying techniques of dimensional reduction, we will just choose the first 1% of the instances of both the train and the testing data of the dataset.

```{r}
# Define the percentage of instances to select
percentage <- 0.01

# Select the first 10% of instances for both train and test datasets
twTrain_small = head(twTrain, round(nrow(twTrain) * percentage))
twTest_small = head(twTest, round(nrow(twTest) * percentage))

#corpus_train_small <- head(corpus_train, round(nrow(corpus_train) * percentage))
corpus_train_small <- head(corpus_train, round(length(corpus_train) * percentage))

#corpus_test_small <- head(corpus_test, round(nrow(corpus_test) * percentage))
corpus_test_small <- head(corpus_test, round(length(corpus_test) * percentage))

```

## 5. Naive Bayes analysis

Now, we are ready to try to classify the data by performing a Naive Bayes analysis.

```{r}
# Turn corpuses into matrices of 0/1 for absence or presence of words.
tw_dtm <- DocumentTermMatrix(clean_corpus)
inspect(tw_dtm[2:10, 50:70])  # Inspect some rows and columns

tw_dtm_train <- tw_dtm[spl,]
tw_dtm_test <- tw_dtm[-spl,]

# Look at words which appear at least five times in the training data set for Twitter
five_times_words_tw <- findFreqTerms(tw_dtm_train, 5)
length(five_times_words_tw)
five_times_words_tw[1:5]

# Create Document-Term Matrices using the selected words for Twitter
tw_dtm_train <- DocumentTermMatrix(corpus_train_small,
                                   control=list(dictionary=five_times_words_tw))

tw_dtm_test <- DocumentTermMatrix(corpus_test_small,
                                  control=list(dictionary=five_times_words_tw))

# Convert 0 and 1 to "No" and "Yes"
convert_count_tw <- function(x){
  y <- ifelse(x > 0, 1, 0)
  y <- factor(y, levels=c(0, 1), labels=c("No", "Yes"))
  y
}

tw_dtm_train <- apply(tw_dtm_train, 2, convert_count_tw)
tw_dtm_train[1:4, 30:35]

tw_dtm_test <- apply(tw_dtm_test, 2, convert_count_tw)
tw_dtm_test[2:10, 50:70]

# Train the classifier
classifier <- naiveBayes(tw_dtm_train, twTrain_small$category)
class(classifier)
```

## 6. Results

Report both in sample (training) results and out of sample (test) results.

Let's start by calculating the in-sample (training) results.

```{r}
# Predictions on training data
train_predictions <- predict(classifier, newdata=tw_dtm_train)

# Confusion matrix for in-sample results
confusion_train <- table(train_predictions, twTrain_small$category)
confusion_train

```

Now, let's calculate the accuracy for the in-sample results:

```{r}
# Accuracy for in-sample results
accuracy_train <- sum(diag(confusion_train)) / sum(confusion_train)
accuracy_train
```

The accuracy in the train set is 91.3%.

Now, let's move on to out-of-sample (test) results:

```{r}
# Use the classifier to make the predictions
test_predictions <- predict(classifier, newdata=tw_dtm_test)

# Confusion matrix for out-of-sample results
confusion_test <- table(test_predictions, twTest_small$category)
confusion_test
```

Calculating the accuracy for the out-of-sample results:

```{r}
# Accuracy for out-of-sample results
accuracy_test <- sum(diag(confusion_test)) / sum(confusion_test)
accuracy_test

```

The accuracy in the test set is 66.3%.

**In-sample (Training) Results:**

-   The confusion matrix for in-sample predictions shows that the model performs well on the training data.

-   The majority of negative (-1) and positive (1) sentiments are correctly classified, with high counts along the diagonal.

-   There are some misclassifications, particularly in predicting neutral (0) sentiments, as indicated by the non-zero values outside the diagonal in the respective row and column.

-   The overall accuracy on the training data is high, with an accuracy of approximately 91.33%.

**Out-of-sample (Test) Results:**

-   The confusion matrix for out-of-sample predictions indicates a less accurate performance compared to the training set.

-   There are noticeable misclassifications across all sentiment categories, with higher counts of misclassifications in predicting negative (-1) and positive (1) sentiments.

-   The accuracy on the test data is lower, approximately 66.26%, suggesting that the model's performance drops when applied to new, unseen data.

**Conclusions:**

1.  **Model Performance on Training Data:**

    -   The model shows strong performance on the training data, achieving a high accuracy of approximately 91.33%.

    -   The model effectively captures patterns present in the training set and can confidently predict sentiment labels.

2.  **Generalization to Test Data:**

    -   The model's performance drops when applied to the test data, indicating some difficulty in generalizing to new, unseen instances.

    -   The lower accuracy on the test set (approximately 66.26%) suggests that the model may not generalize as well to real-world data.

3.  **Areas for Improvement:**

    -   The model seems to struggle with predicting neutral sentiments (0) in both training and test sets, as seen in the confusion matrices.

    -   Further feature engineering or parameter tuning may be explored to enhance the model's ability to distinguish between sentiment categories.

4.  **Consideration for Deployment:**

    -   While the model performs well on the training data, its lower accuracy on the test set raises concerns about its real-world applicability.

    -   Caution should be exercised when deploying the model in a production environment, and further improvements or adjustments may be necessary to enhance generalization.

In summary, while the model demonstrates strong performance on the training data, there is room for improvement in its ability to generalize to new, unseen instances, particularly in correctly classifying neutral sentiments.

## 7. Applying Laplace smoothing (Bayesian naive Bayes)

-   See if results improve

```{r}
# Use the classifier with Laplace smoothing.

B.clas <- naiveBayes(tw_dtm_train, twTrain_small$category,laplace = 1)
class(B.clas)
B.preds <- predict(B.clas, newdata=tw_dtm_test)
table(B.preds, twTest_small$category)
```

## 8. Conclusions and possible improvements
