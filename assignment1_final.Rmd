---
title: "Case Study: Naive Bayes application "
author: "Lara Monteserín Placer"
date: 'UC3M, 2023/24'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)

```

# Assignment 1

### Lara Monteserín Placer

Every day, millions of users express their thoughts and emotions through tweets, creating a vast pool of valuable data. Analyzing the sentiment behind these tweets provides insights into public opinions, attitudes, and feelings.

In this study, our goal is to harness the power of sentiment analysis by building a classifier to predict whether a tweet carries a positive, negative, or neutral sentiment based on the words and expressions used by the users. This will be done through a brief naive Bayes analysis.

Some of the advantages of being able to classify tweets depending on its sentiment are:

1.  **Public Opinion Monitoring:** It offers real-time insights into public opinion on various topics, products, or events.

2.  **Brand Perception:** If performed around a brand on social media, it aids businesses in reinforcing loyalty with positive mentions and managing negative sentiments for effective reputation management.

3.  **Customer Feedback Analysis:** It provides companies with valuable insights into customer experiences, facilitating data-driven improvements in products and services.

4.  **Market Trends and Predictions:** It helps to identify emerging trends and anticipate shifts in public sentiment, empowering in making informed decisions.

5.  **Public Response to Events:** Crucial for public relations and crisis management, it offers a quick and efficient way to gauge how people react to specific events or announcements on social media.

![](foto.avif)

In this case study, we will use a dataset with 162961 instances of tweets published in India during the year 2021.

## 1. Introduction to the data

We took the dataset from Kaggle: <https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset>

The dataset Twitter_Data.csv includes the following 2 variables:

1.  **clean_text**: The complete text from the tweet. The type of the data is character.

2.  **category**: A unique code assigned to each text, indicating the sentiment code. The type of data is integer. There are 3 types of sentiments to classify the tweets in:

    -   Positive: 1

    -   Neutral: 0

    -   Negative: -1

```{r}
#To not show warnings
options(warn = -1)

#Clear the system.
rm(list=ls())

library(tidyverse)
library(MASS)
library(e1071)
library(caret)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)

setwd("C:/Users/laram/Desktop/Todo/UC3M/Third Bimester/Bayesian Learning/Assignment 1")


#For the explanations of the results to be consistent always
set.seed(123)

twitter <- read.csv("Twitter_Data.csv")

# Display column names in the dataset
colnames(twitter)

dim(twitter)
str(twitter)
summary(twitter)
```

## 2. Data cleaning analysis

We prepare the dataset for statistical analysis. We want to ensure that we are working with a reliable and accurate dataset.

Until now, we have 162980 instances.

The 2 columns have null values, 7 in category and 4 in clean_text. As this value is very low compared to the size of the dataset, we will just remove those instances, so we keep 162973 instances.

When first removing duplicates, we find out that there are just 2 duplicated values. Also, if we count the unique values in clean_text, there are only 162969, so these are the values that we keep to perform the statistical analysis.

After that, we turn the tweets into a corpus in order to work with them. From here, we will clean the corpus: put everything in lower case. remove any numbers and punctuation, take out uninformative stop words and remove excess white space.

Finally, we extract the indices (pos_indices, neu_indices and neg_indices) that point to the positions of rows where the category is considered positive, neutral or negative, respectively.

```{r}
# Counting NAs and empty values in each column
na_counts <- colSums(is.na(twitter) | twitter == "")
na_counts <- na_counts[order(-na_counts)]
print(na_counts)

# Removing instances with null values
twitter <- twitter[complete.cases(twitter), ]

#Current number of rows
n_rows <- nrow(twitter)
print(n_rows)

#Creates a logical vector duplicate_rows indicating whether each row in the data frame is a duplicate of a previous row
duplicate_rows <- twitter %>% duplicated.data.frame()

#Retains only the unique rows
library(dplyr)
twitter <- distinct(twitter)

#Number of rows after removing duplicates
n_rows2 <- nrow(twitter)
print(n_rows2)

# Duplicates based on column "clean_text"
twitter <- twitter %>% distinct(clean_text, .keep_all = TRUE)

n_rows3 <- nrow(twitter)
print(n_rows3)

# Turn the messages into a corpus.
corpus <- Corpus(VectorSource(twitter$clean_text))
inspect(corpus[1:5])

## CLEANING

# Put everything in lower case.
clean_corpus <- tm_map(corpus, tolower)
inspect(clean_corpus[1:5])

# Remove any numbers.
clean_corpus <- tm_map(clean_corpus, removeNumbers)
inspect(clean_corpus[1:5])

# Remove punctuation.
clean_corpus <- tm_map(clean_corpus, removePunctuation)
inspect(clean_corpus[1:5])

# Take out uninformative stop words.
stopwords("en")[1:10]
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords("en"))
inspect(clean_corpus[1:5])

# Remove excess white space.
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
inspect(clean_corpus[1:5])

# Check which tweets are positive, which neutral and which are negative.

pos_indices <- which(twitter$category == 1)
pos_indices[1:3]

neu_indices <- which(twitter$category == 0)
neu_indices[1:3]

neg_indices <- which(twitter$category == -1)
neg_indices[1:3]
```

## 3. Separate the data into a training set and a test set

It is always a good idea to separate from the beginning the training set (what the tool is going to see) from the testing set (used only to validate predictions).

```{r}
# split between training and testing sets
spl = createDataPartition(twitter$category, p = 0.8, list = FALSE)

twTrain = twitter[spl,]
twTest = twitter[-spl,]

corpus_train <- clean_corpus[spl]
corpus_test <- clean_corpus[-spl]

#str(twTrain)
#summary(twTrain)


```

## 4. Use word clouds

We plot the three clouds corresponding to the 3 types of sentiment that can be extracted from the tweets, in order to look at the different categories and explain any obvious differences between them.

As we are working with a huge number of instances, we need to establish a big number for the minimum frequency of the words to appear in the word cloud. This is the reason why we have chosen 1500 words as the minimum in all texts together.

For all indices, "Modi" is the most repeated word by far, that is the surname of the Prime Minister of India.

For the positive indices, some words stand out apart from "Modi", just like: "India", "best", "win", "love", "please" "power" or "thanks", "good", "great", "happy".

For the neutral indices, the most repeated words are "congress", "bjp" (that is a political party in India), "vote", "minister", "nation" or "country", that are at the same time common with the positive and the negative indices in some cases.

For the negative indices, the words that stand out the most are "poor", "hate", "people", "fake", "money" or "time".

```{r}
par(mar=c(2,2,2,2))  # Set smaller margins
wordcloud(clean_corpus[pos_indices], min.freq=1500, scale=c(3,.5))
wordcloud(clean_corpus[neu_indices], min.freq=1500, scale=c(3,.5))
wordcloud(clean_corpus[neg_indices], min.freq=1500, scale=c(3,.5))
```

## \* Something to point out

The dataset we have chosen is that big that the function convert_count_tw cannot be applied to the train and the test sets because there is a problem:

*Error: cannot allocate vector of size X Gb*

As this assignment is not about applying techniques of dimensional reduction, we will just choose the first 1% of the instances of both the train and the testing data of the dataset.

```{r}
# Define the percentage of instances to select
percentage <- 0.01

# Select the first 10% of instances for both train and test datasets
twTrain_small = head(twTrain, round(nrow(twTrain) * percentage))
twTest_small = head(twTest, round(nrow(twTest) * percentage))

corpus_train_small <- head(corpus_train, round(length(corpus_train) * percentage))

corpus_test_small <- head(corpus_test, round(length(corpus_test) * percentage))

```

## 5. Naive Bayes analysis

Now, we are ready to try to classify the data by performing a Naive Bayes analysis.

```{r}
# Turn corpuses into matrices of 0/1 for absence or presence of words.
tw_dtm <- DocumentTermMatrix(clean_corpus)
inspect(tw_dtm[2:10, 50:70])  # Inspect some rows and columns

tw_dtm_train <- tw_dtm[spl,]
tw_dtm_test <- tw_dtm[-spl,]

# Look at words which appear at least five times in the training data set for Twitter
five_times_words_tw <- findFreqTerms(tw_dtm_train, 5)
length(five_times_words_tw)
five_times_words_tw[1:5]

# Create Document-Term Matrices using the selected words for Twitter
tw_dtm_train <- DocumentTermMatrix(corpus_train_small,
                                   control=list(dictionary=five_times_words_tw))

tw_dtm_test <- DocumentTermMatrix(corpus_test_small,
                                  control=list(dictionary=five_times_words_tw))

# Convert 0 and 1 to "No" and "Yes"
convert_count_tw <- function(x){
  y <- ifelse(x > 0, 1, 0)
  y <- factor(y, levels=c(0, 1), labels=c("No", "Yes"))
  y
}

tw_dtm_train <- apply(tw_dtm_train, 2, convert_count_tw)
tw_dtm_train[1:4, 30:35]

tw_dtm_test <- apply(tw_dtm_test, 2, convert_count_tw)
tw_dtm_test[2:10, 50:70]

# Train the classifier
classifier <- naiveBayes(tw_dtm_train, twTrain_small$category)
class(classifier)
```

## 6. Results

In this section, we will report both in sample (training) results and out of sample (test) results.

### 6.1 Training results

Let's start by calculating the in-sample (training) results.

```{r}
# Predictions on training data
train_predictions <- predict(classifier, newdata=tw_dtm_train)

# Confusion matrix for in-sample results
confusion_train <- table(train_predictions, twTrain_small$category)
confusion_train

```

Now, let's calculate the accuracy for the in-sample results:

```{r}
# Accuracy for in-sample results
accuracy_train <- sum(diag(confusion_train)) / sum(confusion_train)
accuracy_train
```

The accuracy in the train set is 91.95%.

#### Conclusions

-   The confusion matrix for in-sample predictions shows that the model performs well on the training data.

-   The majority of negative (-1) and positive (1) sentiments are correctly classified, with high counts along the diagonal. There are some misclassifications, particularly in predicting neutral (0) sentiments, as indicated by the non-zero values outside the diagonal in the respective row and column.

-   The overall accuracy on the training data is high, with an accuracy of approximately 91.95%.

### 6.2 Test results

Now, let's move on to out-of-sample (test) results:

```{r}
# Use the classifier to make the predictions
test_predictions <- predict(classifier, newdata=tw_dtm_test)

# Confusion matrix for out-of-sample results
confusion_test <- table(test_predictions, twTest_small$category)
confusion_test
```

Calculating the accuracy for the out-of-sample results:

```{r}
# Accuracy for out-of-sample results
accuracy_test <- sum(diag(confusion_test)) / sum(confusion_test)
accuracy_test

```

The accuracy in the test set is 64.72%.

#### Conclusions

-   The confusion matrix for out-of-sample predictions indicates a less accurate performance compared to the training set.

-   There are noticeable misclassifications across all sentiment categories, with higher counts of misclassifications in predicting negative (-1) and positive (1) sentiments.

-   The accuracy on the test data is lower, approximately 64.72%, suggesting that the model's performance drops when applied to new, unseen data. We will analyse this in the final conclusions (8th step).

### 6.3 Overall results

The model shows strong performance on the training data, achieving a high accuracy of approximately 91.94%. The model effectively captures patterns present in the training set and can confidently predict sentiment labels.

The model's performance drops when applied to the test data, indicating some difficulty in generalizing to new, unseen instances. The lower accuracy on the test set (approximately 64.72%) suggests that the model may not generalize as well to real-world data.

**Areas for Improvement:**

The model seems to struggle with predicting neutral sentiments (0) in both training and test sets, as seen in the confusion matrices. While the model performs well on the training data, its lower accuracy on the test set raises concerns about its real-world applicability.

## 7. Applying Laplace smoothing (Bayesian naive Bayes)

Next, we will apply Laplace Smoothing in order to see if results improve. This helps address the issue of zero probabilities for unseen features in the Naive Bayes model, improving its generalization performance.

```{r}
# Use the classifier with Laplace smoothing.
B.clas <- naiveBayes(tw_dtm_train, twTrain_small$category,laplace = 1)
class(B.clas)
B.preds <- predict(B.clas, newdata=tw_dtm_test)
confusion_laplacian <- table(B.preds, twTest_small$category)
```

```{r}
# Accuracy for out-of-sample results
accuracy_laplacian <- sum(diag(confusion_laplacian)) / sum(confusion_laplacian)
accuracy_laplacian
```

If we set the parameter *laplace = 1,* it seems like the model is not making any predictions for the neutral class (category 0) neither for positive class (1), resulting in all zero counts for those categories. However, we have noticed that the closer to zero we set the parameter, the better confusion matrix we achieve (and the better accuracy). We have created a loop to illustrate it:

```{r}
laplace_values <- c(0.5, 0.25, 0.1)

for (laplace_val in laplace_values) {
  B.clas <- naiveBayes(tw_dtm_train, twTrain_small$category, laplace = laplace_val)
  cat("Laplace Value:", laplace_val, "\n")

  # Predictions on the test data
  B.preds <- predict(B.clas, newdata = tw_dtm_test)
  
  # Confusion matrix for out-of-sample results
  confusion_laplacian <- table(B.preds, twTest_small$category)
  
  # Accuracy for out-of-sample results
  accuracy_laplacian <- sum(diag(confusion_laplacian)) / sum(confusion_laplacian)
  
  cat("Accuracy:", accuracy_laplacian, "\n\n")
}

```

In any case, results are much worse than with no Laplacian Smoothing application, so in the next and final section we will discuss about these results.

## 8. Conclusions and possible improvements

As we are not satisfied with the results, let's try to see the word clouds we represented before but divided in train and test for every class. This way, we will have a clue about whether the test set is representative.

```{r}
# Indices para cada clase en los conjuntos de entrenamiento y prueba
pos_indices_train <- which(twTrain$category == 1)
pos_indices_test <- which(twTest$category == 1)

neu_indices_train <- which(twTrain$category == 0)
neu_indices_test <- which(twTest$category == 0)

neg_indices_train <- which(twTrain$category == -1)
neg_indices_test <- which(twTest$category == -1)
```

```{r}
corpus_train_pos <- clean_corpus[pos_indices_train]
corpus_test_pos <- clean_corpus[pos_indices_test]

corpus_train_neu <- clean_corpus[neu_indices_train]
corpus_test_neu <- clean_corpus[neu_indices_test]

corpus_train_neg <- clean_corpus[neg_indices_train]
corpus_test_neg <- clean_corpus[neg_indices_test]
```

```{r}
par(mfrow=c(3,2))

# Wordcloud for the positive class
wordcloud(corpus_train_pos, min.freq=1500, scale=c(3,.5), main="Train - Positive")
wordcloud(corpus_test_pos, min.freq=1500, scale=c(3,.5), main="Test - Positive")

# Wordcloud for the neutral class
wordcloud(corpus_train_neu, min.freq=1500, scale=c(3,.5), main="Train - Neutral")
wordcloud(corpus_test_neu, min.freq=1500, scale=c(3,.5), main="Test - Neutral")

# Wordcloud for the negative class
wordcloud(corpus_train_neg, min.freq=1500, scale=c(3,.5), main="Train - Negative")
wordcloud(corpus_test_neg, min.freq=1500, scale=c(3,.5), main="Test - Negative")

```

### 8.1 Final conclusions

In view of these new word clouds, we can confirm the previous hypothesis. By far the most repeated word in the dataset is "modi", followed by "india" and "will". When splitting into training and test, as only 20% of the data is destined to test, it makes sense that the cloud of most repeated words is reduced to only those that were the 2 or 3 most repeated words in the training, i.e. the ones mentioned above.

As none of these words can be clearly classified into any class, as they do not necessarily represent a negative, neutral or positive sentiment, it is much more difficult for the classifier to divide the tweets into the appropriate classes, and therefore we see a large drop in the accuracy value in the test compared to the training.

Before placing a random seed at the start of the code, we ran the code several times to see if we could get better results by randomly splitting the training and test sets, but the accuracy values varied by less than 2% in all cases. However we split the data, the test set will always be smaller and the most repeated words will be by far the most repeated words in the whole dataset.

Finally and in relation to results obtained when applying Laplacian Smoothing, we have come to some conclusions that justify why it might not be the most appropriate for our data:

-   **Frequency-based features:** Laplace smoothing is often beneficial when dealing with sparse data or features with low frequencies. In our case, where the most repeated words are "modi," "india," and "will," these high-frequency words may not benefit significantly from Laplace smoothing. The addition of a small constant may not impact their probabilities much.

-   **Dominance of the "top" words:** The dominance of a few highly frequent words ("modi," "india," and "will") may lead to a situation where the majority of the information for classification comes from a small set of words. Laplace smoothing might not contribute significantly in such cases, and the classifier could be heavily influenced by the most frequent words, potentially leading to over-fitting.

-   **Neutral words:** Words like "modi," "india," and "will" may not carry clear sentiment polarity. Naive Bayes assumes feature independence, and if the dominant words are neutral and present in tweets across different sentiment classes, Laplace smoothing might not effectively handle the challenge of distinguishing between classes. This is probably the key reason for the behaviour we have identified.

### 8.3 Possible improvements

Looking at the results, these are some ways that could improve the performance of our classifier and give better results:

1.  **Feature Engineering:** We could look beyond just individual words and check out other features. Some options would be looking at pairs of words (bigrams), sets of three (trigrams), or even more complicated language patterns. This way we could capture a fuller picture of the context and structure in the text. This makes sense as some sentiments might be expressed more clearly in phrases rather than individual words.

2.  **Custom Stopword List:** A different option would be to reevaluate and customize our stopword list. Words like "modi" ,"india" and "will", that are common across all classes, could be excluded from analysis or treated differently.

3.  **Dictionaries:** It could be interesting to include dictionaries that associate words with sentiment scores. It could provide additional information about the sentiment of individual words beyond frequency counts.

4.  **Ensemble Methods:** Considering more advanced techniques or exploring other machine learning algorithms might be beneficial for improving the model's performance on sentiment classification For example, ensemble methods such as Random Forest or Gradient Boosting seem to be good options, as they often improve predictive performance by combining the strengths of multiple models. Also, when using these methods, we could apply hyperparameter tuning.

5.  **Domain-Specific Preprocessing:** Considering domain-specific preprocessing techniques could be a good idea as our dataset is formed by tweets. This way, we can hadle differently hashtags, mentions, and emojis in a way that they all preserve their meaning.
